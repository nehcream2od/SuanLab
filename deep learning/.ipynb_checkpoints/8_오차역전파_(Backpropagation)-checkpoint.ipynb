{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9Db_VsP_m1S"
      },
      "source": [
        "## 오차역전파 (Backpropagation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vdi2PAG_3lW"
      },
      "source": [
        "### 오차역전파 알고리즘\n",
        "- 학습 데이터로 정방향(forward) 연산을 통해 손실함수 값(loss)을 구함\n",
        "\n",
        "- 각 layer별로 역전파학습을 위해 중간값을 저장\n",
        "\n",
        "- 손실함수를 학습 파라미터(가중치, 편향)로 미분하여  \n",
        "  마지막 layer로부터 앞으로 하나씩 연쇄법칙을 이용하여 미분\n",
        "  각 layer를 통과할 때마다 저장된 값을 이용\n",
        "\n",
        "- 오류(error)를 전달하면서 학습 파라미터를 조금씩 갱신\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-o545ezeUuSB"
      },
      "source": [
        "### 오차역전파 학습의 특징\n",
        "- 손실함수를 통한 평가를 한 번만 하고, 연쇄법칙을 이용한 미분을 활용하기 때문에  \n",
        "  학습 소요시간이 매우 단축!\n",
        "\n",
        "- 미분을 위한 중간값을 모두 저장하기 때문에 메모리를 많이 사용\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFoTt-ds_w8C"
      },
      "source": [
        "### 신경망 학습에 있어서 미분가능의 중요성\n",
        "- 경사하강법(Gradient Descent)에서 손실 함수(cost function)의 최소값,  \n",
        "  즉, 최적값을 찾기 위한 방법으로 미분을 활용\n",
        "\n",
        "- 미분을 통해 손실 함수의 학습 매개변수(trainable parameter)를 갱신하여  \n",
        "  모델의 가중치의 최적값을 찾는 과정\n",
        "\n",
        "![](https://i.pinimg.com/originals/5d/13/20/5d1320c7b672710834e63b95a7c1037b.png)\n",
        "\n",
        "<sub>출처: https://www.pinterest.co.kr/pin/424816177350692379/</sub>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ilYRMqLeQf6Z"
      },
      "source": [
        "### 합성함수의 미분 (연쇄법칙, chain rule)\n",
        "\n",
        "## $\\qquad \\frac{d}{dx} [f(g(x))] = f^\\prime(g(x))g^\\prime(x)$  \n",
        " \n",
        "\n",
        "- 여러 개 연속으로 사용가능  \n",
        "  ## $ \\quad \\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial u} \\times \\frac{\\partial u}{\\partial m} \\times \\frac{\\partial m}{\\partial n} \\times \\ ... \\ \\frac{\\partial l}{\\partial k} \\times \\frac{\\partial k}{\\partial g} \\times \\frac{\\partial g}{\\partial x} $\n",
        "- 각각에 대해 편미분 적용가능\n",
        "\n",
        "![](https://cdn-media-1.freecodecamp.org/images/1*_KMMFvRP5X9kC59brI0ykw.png)\n",
        "<sub>출처: https://www.freecodecamp.org/news/demystifying-gradient-descent-and-backpropagation-via-logistic-regression-based-image-classification-9b5526c2ed46/</sub>\n",
        "\n",
        "- **오차역전파의 직관적 이해**\n",
        "  - 학습을 진행하면서, 즉 손실함수의 최소값(minimum)을 찾아가는 과정에서 가중치 또는 편향의 변화에 따라 얼마나 영향을 받는지 알 수 있음\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6NLF8CXiQkuL"
      },
      "source": [
        "#### 합성함수 미분(chain rule) 예제\n",
        "\n",
        "![](https://miro.medium.com/max/1000/1*azqHvbrNsZ8AIZ7H75tbIQ.jpeg)\n",
        "\n",
        "<sub>출처: https://medium.com/spidernitt/breaking-down-neural-networks-an-intuitive-approach-to-backpropagation-3b2ff958794c</sub>\n",
        "\n",
        "  #### $\\quad a=-1, \\ b=3, \\ c=4$,\n",
        "  #### $\\quad x = a + b, \\ y = b + c, \\ f = x * y \\ 일 때$    \n",
        "\n",
        "\n",
        "\n",
        "  ### $\\quad \\begin{matrix}\\frac{\\partial f}{\\partial x} &=& y\\ + \\ x \\ \\frac{\\partial y}{\\partial x} \\\\ &=& (b \\ + \\ c) \\ + \\ (a \\ +\\ b)\\ \\times \\ 0 \\\\  &=& 7 \\end{matrix}$\n",
        "\n",
        "  ### $\\quad \\begin{matrix}\\frac{\\partial f}{\\partial y} &=& x\\ + \\ \\frac{\\partial x}{\\partial y} \\ y \\\\ &=& (a \\ + \\ b) \\ + \\ 0 \\times (b \\ +\\ c) \\\\ &=& 2 \\end{matrix}$\n",
        "\n",
        "   <br>\n",
        "\n",
        "  ### $ \\quad \\begin{matrix} \\frac{\\partial x}{\\partial a} &=& 1 \\ + \\ a \\ \\frac{\\partial b}{\\partial a} \\\\ &=& 1 \\end{matrix} $\n",
        "  ### $ \\quad \\begin{matrix} \\frac{\\partial y}{\\partial c} &=& \\frac{\\partial b}{\\partial c}\\ + 1 \\\\ &=& 1 \\end{matrix} $\n",
        "  \n",
        "  <br>\n",
        "\n",
        "  ### $ \\quad \\begin{matrix} \\frac{\\partial f}{\\partial a} &=& \\frac{\\partial f}{\\partial x} \\times \\frac{\\partial x}{\\partial a} \\\\ &=& y \\times 1 \\\\  &=& 7 \\times 1 = 7 \\\\ &=& 7  \\end{matrix} $\n",
        "    \n",
        "  ### $ \\quad \\begin{matrix} \\frac{\\partial f}{\\partial b} \\\\ &=& \\frac{\\partial x}{\\partial b} \\ y \\ + \\ x \\ \\frac{\\partial y}{\\partial b}  \\\\ &=& 1 \\times 7 + 2 \\times 1  \\\\ &=& 9 \\end{matrix} $\n",
        "  \n",
        "\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PgmdJN0Qtdw"
      },
      "source": [
        "### 덧셈, 곱셈 계층의 역전파\n",
        "- 위 예제를 통해 아래 사항을 알 수 있음\n",
        "\n",
        "  #### 1. $\\quad z = x + y$ 일 때,\n",
        "  ## $\\frac{\\partial z}{\\partial x} = 1, \\frac{\\partial z}{\\partial y} = 1 $\n",
        "\n",
        "  #### 2. $\\quad t = xy$ 일 때,\n",
        "  ## $\\frac{\\partial t}{\\partial x} = y, \\frac{\\partial t}{\\partial y} = x$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hShSxvW5WMqi"
      },
      "outputs": [],
      "source": [
        "class Mul:\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        result = x * y\n",
        "        return result\n",
        "\n",
        "    def backward(self, dresult):\n",
        "        dx = dresult * self.y\n",
        "        dy = dresult * self.x\n",
        "        return dx, dy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1bovOx6UQvLP"
      },
      "outputs": [],
      "source": [
        "class Add:\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        result = x + y\n",
        "        return result\n",
        "\n",
        "    def backward(self, dresult):\n",
        "        dx = dresult * 1\n",
        "        dy = dresult * 1\n",
        "        return dx, dy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "e3RawqlUQwp0"
      },
      "outputs": [],
      "source": [
        "a, b, c = -1, 3, 4\n",
        "x = Add()\n",
        "y = Add()\n",
        "f = Mul()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "soyk-HWiSnwh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "7\n",
            "14\n"
          ]
        }
      ],
      "source": [
        "x_result = x.forward(a, b)\n",
        "y_result = y.forward(b, c)\n",
        "\n",
        "print(x_result)\n",
        "print(y_result)\n",
        "print(f.forward(x_result, y_result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FhPPrFIqSpq1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7 2\n",
            "7\n",
            "9\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "dresult = 1\n",
        "dx_mul, dy_mul = f.backward(dresult)\n",
        "\n",
        "da_add, db_add_1 = x.backward(dx_mul)\n",
        "db_add_2, dc_add = y.backward(dy_mul)\n",
        "\n",
        "print(dx_mul, dy_mul)\n",
        "print(da_add)\n",
        "print(db_add_1 + db_add_2)\n",
        "print(dc_add)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMfl0J1uWgiY"
      },
      "source": [
        "![](https://miro.medium.com/max/2000/1*U3mVDYuvnaLhJzIFw_d5qQ.png)\n",
        "<sub>출처: https://medium.com/spidernitt/breaking-down-neural-networks-an-intuitive-approach-to-backpropagation-3b2ff958794c</sub>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byODNRUF5fbv"
      },
      "source": [
        "### 활성화 함수(Activation)에서의 역전파"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OkRurUgm5hqn"
      },
      "source": [
        "#### 시그모이드(Sigmoid) 함수\n",
        "\n",
        "![](https://media.geeksforgeeks.org/wp-content/uploads/20190911181329/Screenshot-2019-09-11-18.05.46.png)\n",
        "\n",
        "<sub>출처: https://www.geeksforgeeks.org/implement-sigmoid-function-using-numpy/</sub>\n",
        "\n",
        "- 수식 \n",
        "  # $\\quad y = \\frac{1}{1 + e^{-x}} $일 때,\n",
        "\n",
        "  ## $\\quad \\begin{matrix}y' &=& (\\frac{1}{1 + e^{-x}})' \\\\ &=& \\frac{-1}{(1 + e^{-x})^2}\\ \\times \\ (-e^{-x}) \\\\ &=& \\frac{1}{1 + e^{-x}} \\ \\times \\ \\frac{e^{-x}}{1 + e^{-x}} \\\\ &=& \\frac{1}{1 + e^{-x}} \\ \\times \\ (1 - \\frac{1}{1 + e^{-x}}) \\\\ &=& y\\ (1\\ - \\ y)\\end{matrix}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DNWIw7ElVhLK"
      },
      "outputs": [],
      "source": [
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = 1 / (1 + exp(-x))\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.dout\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QWZ247xo5mtv"
      },
      "source": [
        "#### ReLU 함수\n",
        "\n",
        "![](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png)\n",
        "\n",
        "<sub>출처: https://machinelearningmastery.com/rectified-linear-activation-function-for-deep-learning-neural-networks/</sub>\n",
        "\n",
        "\n",
        "- 수식  \n",
        "\n",
        "  ### $\\qquad y= \\begin{cases} x & (x \\ge 0)  \\\\ 0 & (x < 0) \\end{cases}$ 일 때,\n",
        "\n",
        "  <br>\n",
        "\n",
        "  ### $\\qquad \\frac{\\partial y}{\\partial x}= \\begin{cases} 1 & (x \\ge 0)  \\\\ 0 & (x < 0) \\end{cases}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hHcPmAzh5nOZ"
      },
      "outputs": [],
      "source": [
        "class ReLU:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = x < 0\n",
        "        out = x.copy()\n",
        "        out[x < 0] = 0\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "        return dx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH_kQzedJFfw"
      },
      "source": [
        "### 행렬 연산에 대한 역전파\n",
        "\n",
        "# $\\qquad Y = X \\bullet W + B$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-SedVrpJILw"
      },
      "source": [
        "#### 순전파(forward)\n",
        "  \n",
        "  - 형상(shape)을 맞춰줘야함\n",
        "  - 앞서 봤던 곱셈, 덧셈 계층을 합친 형태"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QKIO7EzSJGD1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3,)\n",
            "(3, 2)\n",
            "(2,)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "X = np.random.rand(3)\n",
        "W = np.random.rand(3, 2)\n",
        "B = np.random.rand(2)\n",
        "\n",
        "print(X.shape)\n",
        "print(W.shape)\n",
        "print(B.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Tqvl76fFJNfU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(2,)\n"
          ]
        }
      ],
      "source": [
        "Y = np.dot(X, W) + B\n",
        "print(Y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04GHZHAiJUGl"
      },
      "source": [
        "#### 역전파(1)\n",
        "\n",
        "##  $\\  Y = X \\bullet W$\n",
        "- $X :\\ \\ (2,\\ )$\n",
        "\n",
        "- $W :\\ \\ (2,\\ 3)$\n",
        "\n",
        "- $X \\bullet W :\\ \\ (3,\\ )$\n",
        "\n",
        "- $\\frac{\\partial L}{\\partial Y} :\\ \\ (3,\\ )$\n",
        "\n",
        "- $\\frac{\\partial L}{\\partial X} = \\frac{\\partial L}{\\partial Y}\\bullet W^T ,\\ (2,\\ )$\n",
        "\n",
        "- $\\frac{\\partial L}{\\partial W} = X^T \\bullet \\frac{\\partial L}{\\partial Y} ,\\ (2,\\ 3)$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DCruMHCGJSHK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X\n",
            "[-0.32290928 -0.05256362]\n",
            "W\n",
            "[[-0.40510512 -0.97400109 -0.18885879]\n",
            " [ 0.4899666   0.08994526 -1.05223754]]\n",
            "Y\n",
            "[0.10505779 0.30978615 0.11629367]\n"
          ]
        }
      ],
      "source": [
        "X = np.random.randn(2)\n",
        "W = np.random.randn(2, 3)\n",
        "Y = np.dot(X, W)\n",
        "\n",
        "print(\"X\\n{}\".format(X))\n",
        "print(\"W\\n{}\".format(W))\n",
        "print(\"Y\\n{}\".format(Y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "TEvyFXedJWC7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dL_dY\n",
            "[-2.422077    1.46254484 -0.47627074]\n",
            "dL_dX\n",
            "[-0.35337657 -0.5540379 ]\n",
            "dL_dW\n",
            "[[ 0.78211115 -0.47226931  0.15379224]\n",
            " [ 0.12731313 -0.07687665  0.02503451]]\n"
          ]
        }
      ],
      "source": [
        "dL_dY = np.random.randn(3)\n",
        "dL_dX = np.dot(dL_dY, W.T)\n",
        "dL_dW = np.dot(X.reshape(-1, 1), dL_dY.reshape(1, -1))\n",
        "\n",
        "print(\"dL_dY\\n{}\".format(dL_dY))\n",
        "print(\"dL_dX\\n{}\".format(dL_dX))\n",
        "print(\"dL_dW\\n{}\".format(dL_dW))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuOVqySWJauS"
      },
      "source": [
        "#### 역전파(2)\n",
        "\n",
        "## $\\ (2)\\  Y = X \\bullet W + B$\n",
        "- $X, W$는 위와 동일\n",
        "\n",
        "- $B: \\ (3, )$\n",
        "\n",
        "- $\\frac{\\partial L}{\\partial B} = \\frac{\\partial L}{\\partial Y}, \\ (3,\\ )$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "D_Jb9sY8JYGB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-1.67738587 -0.57994703  2.78263485]\n"
          ]
        }
      ],
      "source": [
        "X = np.random.randn(2)\n",
        "W = np.random.randn(2, 3)\n",
        "B = np.random.randn(3)\n",
        "Y = np.dot(X, W) + B\n",
        "print(Y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "frdvKz8oJcoZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dL_dY\n",
            "[-0.75781509  0.79300283  0.79986044]\n",
            "dL_dX\n",
            "[-3.52276319  1.04340116]\n",
            "dL_dW\n",
            "[[-0.18277653  0.19126342  0.1929174 ]\n",
            " [-0.55084984  0.57642753  0.58141228]]\n",
            "dL_dB\n",
            "[-0.75781509  0.79300283  0.79986044]\n"
          ]
        }
      ],
      "source": [
        "dL_dY = np.random.randn(3)\n",
        "dL_dX = np.dot(dL_dY, W.T)\n",
        "dL_dW = np.dot(X.reshape(-1, 1), dL_dY.reshape(1, -1))\n",
        "dL_dB = dL_dY\n",
        "\n",
        "print(\"dL_dY\\n{}\".format(dL_dY))\n",
        "print(\"dL_dX\\n{}\".format(dL_dX))\n",
        "print(\"dL_dW\\n{}\".format(dL_dW))\n",
        "print(\"dL_dB\\n{}\".format(dL_dB))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FrBSUsRJgFl"
      },
      "source": [
        "#### 배치용 행렬 내적 계층\n",
        "- N개의 데이터에 대해,  \n",
        "# $\\qquad Y = X \\bullet W + B$\n",
        "\n",
        "  - $X : \\quad  (N,\\ 3)$\n",
        "\n",
        "  - $W : \\quad  (3,\\ 2)$\n",
        "\n",
        "  - $B : \\quad  (2,\\ )$\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "AoWx5fDnJd-2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 3)\n",
            "(3, 2)\n",
            "(2,)\n",
            "X\n",
            "[[0.56745771 0.1346261  0.53053782]\n",
            " [0.78383167 0.14200273 0.66770188]\n",
            " [0.58133541 0.28687527 0.76343372]\n",
            " [0.04948798 0.75213437 0.01850882]]\n",
            "W\n",
            "[[0.11671341 0.57701304]\n",
            " [0.21790798 0.35984592]\n",
            " [0.39454288 0.54195193]]\n",
            "Y\n",
            "[-1.67738587 -0.57994703  2.78263485]\n"
          ]
        }
      ],
      "source": [
        "X = np.random.rand(4, 3)\n",
        "W = np.random.rand(3, 2)\n",
        "B = np.random.rand(2)\n",
        "\n",
        "print(X.shape)\n",
        "print(W.shape)\n",
        "print(B.shape)\n",
        "\n",
        "print(\"X\\n{}\".format(X))\n",
        "print(\"W\\n{}\".format(W))\n",
        "print(\"Y\\n{}\".format(Y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EqhSX54BJmPQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(4, 2)\n",
            "Y\n",
            "[[1.13840145 1.6476828 ]\n",
            " [1.21937972 1.84952417]\n",
            " [1.26508489 1.83669504]\n",
            " [1.01049001 1.29352023]]\n"
          ]
        }
      ],
      "source": [
        "Y = np.dot(X, W) + B\n",
        "\n",
        "print(Y.shape)\n",
        "print(\"Y\\n{}\".format(Y))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_1IOOfhuJqF-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dL_dY\n",
            "[[ 1.49362808 -0.53689438]\n",
            " [ 0.19952377  0.19100265]\n",
            " [-0.17330375 -0.330703  ]\n",
            " [ 0.91412754 -1.41596423]]\n",
            "dL_dX\n",
            "[[-0.13546863  0.13227422  0.29832937]\n",
            " [ 0.13349812  0.11220934  0.18223494]\n",
            " [-0.21104682 -0.15676639 -0.24760089]\n",
            " [-0.71033888 -0.31033327 -0.40672204]]\n",
            "dL_dW\n",
            "[[ 0.94845453 -0.4172735 ]\n",
            " [ 0.86724442 -1.20502297]\n",
            " [ 0.81026208 -0.4359876 ]]\n",
            "dL_dB\n",
            "[ 2.43397563 -2.09255896]\n"
          ]
        }
      ],
      "source": [
        "dL_dY = np.random.randn(4, 2)\n",
        "dL_dX = np.dot(dL_dY, W.T)\n",
        "dL_dW = np.dot(X.T, dL_dY)\n",
        "dL_dB = np.sum(dL_dY, axis=0)\n",
        "\n",
        "print(\"dL_dY\\n{}\".format(dL_dY))\n",
        "print(\"dL_dX\\n{}\".format(dL_dX))\n",
        "print(\"dL_dW\\n{}\".format(dL_dW))\n",
        "print(\"dL_dB\\n{}\".format(dL_dB))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "TOKsN2NHJrqT"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self):\n",
        "        self.W = np.random.randn(3, 2)\n",
        "        self.b = np.random.randn(2)\n",
        "        self.x = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        out = np.dot(x, self.W) + self.b\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "YjIYrXGaJtEz"
      },
      "outputs": [],
      "source": [
        "np.random.seed(111)\n",
        "\n",
        "layer = Layer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "twG43EylJvC-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-0.60469438 -0.83761226]\n",
            " [-0.7345356  -0.5993055 ]]\n"
          ]
        }
      ],
      "source": [
        "X = np.random.rand(2, 3)\n",
        "Y = layer.forward(X)\n",
        "\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CrCDxHw5Jw3S"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 0.3637152  -0.60728509  0.86104877]\n",
            " [ 0.60252002 -0.88628947  0.85381569]]\n"
          ]
        }
      ],
      "source": [
        "dout = Y\n",
        "dout_dx = layer.backward(dout)\n",
        "\n",
        "print(dout_dx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bz-0zFPpJ3Jz"
      },
      "source": [
        "### MNIST 분류 with 역전파\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eW3InPxBJ6qo"
      },
      "source": [
        "#### Modules Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ZKtAhkm4J2Z3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/datascience/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
            "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
        "from collections import OrderedDict\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1_2_jGMKGiT"
      },
      "source": [
        "#### 데이터 로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "S3SQq80sJ8mR"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "num_classes = 10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mv_pjKdxKKIy"
      },
      "source": [
        "#### 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "B95lI86tKIyY"
      },
      "outputs": [],
      "source": [
        "X_train, X_test = X_train.reshape(-1, 28 * 28).astype(np.float32), X_test.reshape(\n",
        "    -1, 28 * 28\n",
        ").astype(np.float32)\n",
        "\n",
        "X_train /= .255\n",
        "X_test /= .255\n",
        "\n",
        "y_train = np.eye(num_classes)[y_train]\n",
        "y_test = np.eye(num_classes)[y_test]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "ar7DZIlMKLjJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(60000, 10)\n",
            "(10000, 784)\n",
            "(10000, 10)\n"
          ]
        }
      ],
      "source": [
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oye4pC82KPbg"
      },
      "source": [
        "#### Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "rFeXORJjKM7s"
      },
      "outputs": [],
      "source": [
        "epochs = 1000\n",
        "learning_rate = 1e-3\n",
        "batch_size = 100\n",
        "train_size = X_train.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIdzuyRDKSm0"
      },
      "source": [
        "#### Util Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "8E2_fN1dKRzv"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - np.max(x, axis=0)\n",
        "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "        return y.T\n",
        "\n",
        "    x = x - np.max(x)\n",
        "    return np.exp(x) / np.sum(np.exp(x))\n",
        "\n",
        "\n",
        "def mean_squared_error(pred_y, true_y):\n",
        "    return np.mean(np.square(pred_y - true_y))\n",
        "\n",
        "\n",
        "def cross_entropy_error(pred_y, true_y):\n",
        "    if pred_y.ndim == 1:\n",
        "        true_y = true_y.reshape(1, true_y.size)\n",
        "        pred_y = pred_y.reshape(1, pred_y.size)\n",
        "\n",
        "    if true_y.size == pred_y.size:\n",
        "        true_y = true_y.argmax(axis=1)\n",
        "\n",
        "    batch_size = pred_y.shape[0]\n",
        "    return -np.sum(np.log(pred_y[np.arange(batch_size), true_y]) + 1e-7) / batch_size\n",
        "\n",
        "\n",
        "def softmax_loss(X, true_y):\n",
        "    pred_y = softmax(X)\n",
        "    return cross_entropy_error(pred_y, true_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBvWUNh-KY2R"
      },
      "source": [
        "#### Util Classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc0sLXWJs8Dq"
      },
      "source": [
        "##### ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "ej2m6Robs-uJ"
      },
      "outputs": [],
      "source": [
        "class ReLU:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = x < 0\n",
        "        out = x.copy()\n",
        "        out[x < 0] = 0\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "        return dx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RucLfG0EtYcf"
      },
      "source": [
        "##### Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "TWoPmtpNtZ__"
      },
      "outputs": [],
      "source": [
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = 1 / (1 + np.exp(-x))\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.dout\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZZeNtd-tuM5"
      },
      "source": [
        "##### Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "NEcCZ4bFtzgq"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        self.x = None\n",
        "        self.origin_x_shape = None\n",
        "\n",
        "        self.dL_dW = None\n",
        "        self.dL_db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.origin_x_shape = x.shape\n",
        "\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        self.x = x\n",
        "        out = np.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T)\n",
        "        self.dL_dW = np.dot(self.x.T, dout)\n",
        "        self.dL_db = np.sum(dout, axis=0)\n",
        "        dx = dx.reshape(*self.origin_x_shape)\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1Yqp7kVvOoD"
      },
      "source": [
        "#### Softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "0RXBVEMdvP9r"
      },
      "outputs": [],
      "source": [
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None\n",
        "        self.t = None\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        return self.loss\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims=True)\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "\n",
        "        if self.t.size == self.y.size:\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copu()\n",
        "            dx[np.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "        return dx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "sX4I-bKfuRaq"
      },
      "outputs": [],
      "source": [
        "class MyModel:\n",
        "    def __init__(self, input_size, hidden_size_list, output_size, activation=\"relu\"):\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.hidden_size_list = hidden_size_list\n",
        "        self.hidden_layer_num = len(hidden_size_list)\n",
        "        self.params = {}\n",
        "\n",
        "        self.__init_weights(activation)\n",
        "\n",
        "        activation_layer = {\"sigmoid\": Sigmoid, \"relu\": ReLU}\n",
        "        self.layers = OrderedDict()\n",
        "        for idx in range(1, self.hidden_layer_num + 1):\n",
        "            self.layers[\"Layer\" + str(idx)] = Layer(\n",
        "                self.params[\"W\" + str(idx)], self.params[\"b\" + str(idx)]\n",
        "            )\n",
        "            self.layers[\"Activation_function\" + str(idx)] = activation_layer[\n",
        "                activation\n",
        "            ]()\n",
        "\n",
        "        idx = self.hidden_layer_num + 1\n",
        "\n",
        "        self.layers[\"Layer\" + str(idx)] = Layer(\n",
        "            self.params[\"W\" + str(idx)], self.params[\"b\" + str(idx)]\n",
        "        )\n",
        "\n",
        "        self.last_layer = Softmax()\n",
        "\n",
        "    def __init_weights(self, activation):\n",
        "        weight_std = None\n",
        "        all_size_list = [self.input_size] + self.hidden_size_list + [self.output_size]\n",
        "        for idx in range(1, len(all_size_list)):\n",
        "            if activation.lower() == \"relu\":\n",
        "                weight_std = np.sqrt(2.0 / self.input_size)\n",
        "            elif activation.lower() == \"sigmoid\":\n",
        "                weight_std = np.sqrt(1.0 / self.input_size)\n",
        "\n",
        "            self.params[\"W\" + str(idx)] = weight_std * np.random.randn(\n",
        "                all_size_list[idx - 1], all_size_list[idx]\n",
        "            )\n",
        "            self.params[\"b\" + str(idx)] = np.random.randn(all_size_list[idx])\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "        return X\n",
        "\n",
        "    def loss(self, x, true_y):\n",
        "        pred_y = self.predict(x)\n",
        "\n",
        "        return self.last_layer.forward(pred_y, true_y)\n",
        "\n",
        "    def accuracy(self, x, true_y):\n",
        "        pred_y = self.predict(x)\n",
        "        pred_y = np.argmax(pred_y, axis=1)\n",
        "\n",
        "        if true_y.ndim != 1:\n",
        "            true_y = np.argmax(true_y, axis=1)\n",
        "        accuracy = np.sum(pred_y == true_y) / float(x.shape[0])\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        self.loss(x, t)\n",
        "\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in range(1, self.hidden_layer_num + 2):\n",
        "            grads[\"W\" + str(idx)] = self.layers[\"Layer\" + str(idx)].dL_dW\n",
        "            grads[\"b\" + str(idx)] = self.layers[\"Layer\" + str(idx)].dL_db\n",
        "        return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwyNo3TsyDZR"
      },
      "source": [
        "#### 모델 생성 및 학습"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "Du5naLufxMvv"
      },
      "outputs": [],
      "source": [
        "model = MyModel(28 * 28, [100, 64, 32], 10, activation=\"relu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "Rb0HhW9x0mrN"
      },
      "outputs": [],
      "source": [
        "train_lost_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "VV5PciwAxM6s"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "arrays used as indices must be of integer (or boolean) type",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[119], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m x_batch \u001b[39m=\u001b[39m X_train[batch_mask]\n\u001b[1;32m      4\u001b[0m y_batch \u001b[39m=\u001b[39m y_train[batch_mask]\n\u001b[0;32m----> 6\u001b[0m grad \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgradient(x_batch, y_batch)\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mparams\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m      9\u001b[0m     model\u001b[39m.\u001b[39mparams[key] \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m grad[key]\n",
            "Cell \u001b[0;32mIn[116], line 62\u001b[0m, in \u001b[0;36mMyModel.gradient\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgradient\u001b[39m(\u001b[39mself\u001b[39m, x, t):\n\u001b[0;32m---> 62\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloss(x, t)\n\u001b[1;32m     64\u001b[0m     dout \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     65\u001b[0m     dout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_layer\u001b[39m.\u001b[39mbackward(dout)\n",
            "Cell \u001b[0;32mIn[116], line 51\u001b[0m, in \u001b[0;36mMyModel.loss\u001b[0;34m(self, x, true_y)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mloss\u001b[39m(\u001b[39mself\u001b[39m, x, true_y):\n\u001b[1;32m     49\u001b[0m     pred_y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(x)\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlast_layer\u001b[39m.\u001b[39;49mforward(pred_y, true_y)\n",
            "Cell \u001b[0;32mIn[115], line 10\u001b[0m, in \u001b[0;36mSoftmax.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt \u001b[39m=\u001b[39m t\n\u001b[1;32m      9\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39my \u001b[39m=\u001b[39m softmax(x)\n\u001b[0;32m---> 10\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss \u001b[39m=\u001b[39m cross_entropy_error(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mt)\n\u001b[1;32m     11\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss\n\u001b[1;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mexp(x) \u001b[39m/\u001b[39m np\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mexp(x), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdims\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "Cell \u001b[0;32mIn[111], line 25\u001b[0m, in \u001b[0;36mcross_entropy_error\u001b[0;34m(pred_y, true_y)\u001b[0m\n\u001b[1;32m     22\u001b[0m     true_y \u001b[39m=\u001b[39m true_y\u001b[39m.\u001b[39margmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     24\u001b[0m batch_size \u001b[39m=\u001b[39m pred_y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 25\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39msum(np\u001b[39m.\u001b[39mlog(pred_y[np\u001b[39m.\u001b[39;49marange(batch_size), true_y]) \u001b[39m+\u001b[39m \u001b[39m1e-7\u001b[39m) \u001b[39m/\u001b[39m batch_size\n",
            "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
          ]
        }
      ],
      "source": [
        "for epoch in range(epochs):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = X_train[batch_mask]\n",
        "    y_batch = y_train[batch_mask]\n",
        "\n",
        "    grad = model.gradient(x_batch, y_batch)\n",
        "\n",
        "    for key in model.params.keys():\n",
        "        model.params[key] -= learning_rate * grad[key]\n",
        "\n",
        "    loss = model.loss(x_batch, y_batch)\n",
        "    train_lost_list.append(loss)\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        train_acc = model.accuracy(X_train, y_train)\n",
        "        test_acc = model.accuracy(X_test, y_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(\n",
        "            \"Epoch: {} Train Accuracy: {:.4f} Test Accuracy: {:.4f}\".format(\n",
        "                epoch + 1, train_acc, test_acc\n",
        "            )\n",
        "        )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "datascience",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "vscode": {
      "interpreter": {
        "hash": "7c15afef78f50668981ba711ec2661f2a5596a86ec48569938de9a5d0b5f4743"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
